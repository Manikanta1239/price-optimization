{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99d0b75-277d-4c75-96d0-910e38beef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in d:\\venv\\lib\\site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\venv\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: click in d:\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\venv\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in d:\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: python-docx, pypdf2, nltk\n",
      "Successfully installed nltk-3.9.1 pypdf2-3.0.1 python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx pypdf2 nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3267ecb0-9352-45f3-838d-9e03280f56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.44\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RelvanceScore(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.punctuations = string.punctuation + string.digits + '’' + '“' + '”'\n",
    "        self.Lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.Vectorizer = TfidfVectorizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def Exception(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except (Exception, FileNotFoundError, ValueError, NameError, TypeError) as e:\n",
    "                return e\n",
    "        return wrapper\n",
    "    \n",
    "    @Exception\n",
    "    def document_parser(self, document) -> str:\n",
    "        if document.endswith('.docx'):\n",
    "            self.doc = Document(document)\n",
    "            text: str = ''\n",
    "            for para in self.doc.paragraphs:\n",
    "                text += para.text\n",
    "            return text\n",
    "        elif document.endswith('.pdf'):\n",
    "            self.PDF = PdfReader(document)\n",
    "            text = ''\n",
    "            for page in range(len(self.PDF.pages)):\n",
    "                text += self.PDF.pages[page].extract_text()\n",
    "            return text\n",
    "        elif document.endswith('.txt'):\n",
    "            with open(document, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "            return text\n",
    "        else:\n",
    "            return 'Invalid File Format'\n",
    "    @Exception\n",
    "    def data_preprocessing(self, document: str):\n",
    "        for token in word_tokenize(document):\n",
    "            if token not in self.stopwords and token not in self.punctuations and len(token) > 1:\n",
    "                sub_tokens = re.split(r'[_/\\\\\\-\\|]', token)\n",
    "                for sub_token in sub_tokens:\n",
    "                    if sub_token not in self.stopwords and len(sub_token) > 1:\n",
    "                        yield self.Lemmatizer.lemmatize(sub_token.lower())\n",
    "\n",
    "    @Exception\n",
    "    def get_score(self, RESUME: list, JD: list):\n",
    "        # Combine tokens into strings\n",
    "        CORPUS = [' '.join(RESUME), ' '.join(JD)]\n",
    "        TFIDF_MATRIX = self.Vectorizer.fit_transform(CORPUS)\n",
    "        SIMILARITY = cosine_similarity(TFIDF_MATRIX[0], TFIDF_MATRIX[1])\n",
    "        return round(abs(SIMILARITY[0][0] * 100),2)\n",
    "\n",
    "    def main(self) -> Exception:\n",
    "        RESUME = \"Resume.pdf\"\n",
    "        JD = \"JD.txt\"\n",
    "        RESUME = self.document_parser(RESUME)\n",
    "        JD = self.document_parser(JD)\n",
    "        RESUME = list(self.data_preprocessing(RESUME))\n",
    "        JD = list(self.data_preprocessing(JD))\n",
    "        return self.get_score(RESUME, JD)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = RelvanceScore()\n",
    "    score = obj.main()\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb21082-6e00-4959-a281-e110df88225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547e8aef-bff8-4b7b-a1d5-caae8279ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume Relevance Score: 9.44%\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class RelevanceScore:\n",
    "    def __init__(self) -> None:\n",
    "        self.punctuations = string.punctuation + string.digits + '’' + '“' + '”'\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def exception_handler(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except (Exception, FileNotFoundError, ValueError, NameError, TypeError) as e:\n",
    "                return str(e)  # Return error message as string\n",
    "        return wrapper\n",
    "    \n",
    "    @exception_handler\n",
    "    def document_parser(self, document_path) -> str:\n",
    "        if document_path.endswith('.docx'):\n",
    "            doc = Document(document_path)\n",
    "            return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "        elif document_path.endswith('.pdf'):\n",
    "            pdf = PdfReader(document_path)\n",
    "            return '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "        elif document_path.endswith('.txt'):\n",
    "            with open(document_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "        else:\n",
    "            return 'Invalid File Format'\n",
    "    \n",
    "    @exception_handler\n",
    "    def data_preprocessing(self, document: str):\n",
    "        processed_tokens = []\n",
    "        for token in word_tokenize(document):\n",
    "            if token not in self.stopwords and token not in self.punctuations and len(token) > 1:\n",
    "                sub_tokens = re.split(r'[_/\\\\\\-\\|]', token)\n",
    "                for sub_token in sub_tokens:\n",
    "                    if sub_token not in self.stopwords and len(sub_token) > 1:\n",
    "                        processed_tokens.append(self.lemmatizer.lemmatize(sub_token.lower()))\n",
    "        return processed_tokens\n",
    "\n",
    "    @exception_handler\n",
    "    def get_score(self, resume_tokens: list, jd_tokens: list):\n",
    "        corpus = [' '.join(resume_tokens), ' '.join(jd_tokens)]\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(corpus)\n",
    "        similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "        return round(abs(similarity[0][0] * 100), 2)\n",
    "\n",
    "    def main(self, resume_path: str, jd_path: str):\n",
    "        resume_text = self.document_parser(resume_path)\n",
    "        jd_text = self.document_parser(jd_path)\n",
    "\n",
    "        if isinstance(resume_text, str) and isinstance(jd_text, str):\n",
    "            resume_tokens = self.data_preprocessing(resume_text)\n",
    "            jd_tokens = self.data_preprocessing(jd_text)\n",
    "            return self.get_score(resume_tokens, jd_tokens)\n",
    "        return \"Error in processing documents.\"\n",
    "\n",
    "# File paths (update these paths based on your Jupyter Notebook environment)\n",
    "resume_path = \"Resume.pdf\"  # Update with the actual resume file path\n",
    "jd_path = \"JD.txt\"  # Update with the actual job description file path\n",
    "\n",
    "# Run the Relevance Score Calculation\n",
    "obj = RelevanceScore()\n",
    "score = obj.main(resume_path, jd_path)\n",
    "print(f\"Resume Relevance Score: {score}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711cef33-1234-4279-a501-5bcb46146a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
